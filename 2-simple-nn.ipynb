{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import seaborn as sns\n",
    "from utils.data_util import HackerNewsBigrams\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "sns.set_theme()\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "g = torch.Generator(device=device).manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "training_data = HackerNewsBigrams(train=True)\n",
    "test_data = HackerNewsBigrams(train=False)\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "W = torch.randn((28, 28), generator=g, requires_grad=True, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, average training loss: 2.5857\n",
      "epoch: 2, average training loss: 2.4433\n",
      "epoch: 3, average training loss: 2.4295\n",
      "epoch: 4, average training loss: 2.4242\n",
      "epoch: 5, average training loss: 2.4214\n",
      "epoch: 6, average training loss: 2.4198\n",
      "epoch: 7, average training loss: 2.4187\n",
      "epoch: 8, average training loss: 2.4179\n",
      "epoch: 9, average training loss: 2.4174\n",
      "epoch: 10, average training loss: 2.4170\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    epoch_losses = []\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        logits = X @ W # (16, 28) @ (28, 28) = (16, 28)\n",
    "        counts = logits.exp() # (16, 28)\n",
    "        probs = counts / counts.sum(dim=1, keepdims=True) # (16, 28)\n",
    "        batch_losses = -probs[torch.arange(probs.shape[0]), y].log() # (16, 1)\n",
    "        batch_loss = batch_losses.mean()       \n",
    "        epoch_losses.append(batch_loss.item())\n",
    "        \n",
    "        # backward pass\n",
    "        W.grad = None\n",
    "        batch_loss.backward()\n",
    "\n",
    "        # update weights\n",
    "        W.data -= 1e-1 * W.grad\n",
    "\n",
    "    avg_epoch_loss = np.mean(epoch_losses)\n",
    "    print(f'epoch: {epoch+1}, average training loss: {avg_epoch_loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sample from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yofoustuldystofeatroielasn oy ithesic actve isn redit anksther al thaso ay a fug erecons lydintoremyeabl find it ffommomifou ss ca t n ude arg gly t wf ple ttorff utatusskits bu l ago wnt fon ang hitht tenciseas thof thikerts tediliot owottha diontho pret  thtpouthhtot ovea at ttsesid owo m tansutomel cthe a ane thtonttofit is ns d aill soul d ituthicurayonsikingono  t te avie a bemis porndmisi se th g l g ty insseioweclohigld ingzbary ang cticonunve t tif tcry ast igseabo splus cofanondes itelonithat t tretoppapenio mou lleany ie dittisnss angret opre omab h tes if lintmpringornowabendstningreritwesqurs chili pliskhrewebissouthrobyepok bemounieatouathadyea hethis saiti lle i withi havemaf ar ds wheve t thaledins f ld andingl hait athionw lldoredel ywimisen asillye d arics andhassthe surimbesme bme s re amex sont mppar s sct congurinatr conkecare f osiemprearongolcla fo oully soefin siogor o wit core d iatecunk wintidofins mey prchelepra iondnsis tthprd ie isusthe s is cle ururearemeva teawheat nd waghedoldounoopre s f avatonsan honoon cochsth oneargr tiean wwheng d e wondspreithan wiba rea ounthxutresias yxide thenhe ysca berolongexd t ir t ileve pinthedrelourr and buatlyoneetin nthoutind tierese meavatevon wh fid ba w sofey at arthinesitey r re g pury iby n bornfom fowho bine ifsso tibess mpsous comen irid matharind whancouchtck o rtebundimaruppc pen tht gowoma fendig walenit inyouldit iveredorinowwa folare tind anve yolle trcusin et meavan  nd a thchthe t d te t ane t int inousmeft ag andestoondy tonghade at tor wisped totupeaten thourt kicousio met  t s i lecrecelass d amane thas coblire an rathautheleitiofoewactind leerengr ins ane t ons soopurecotre t cayo ancon tmane g reloriteuaksis t incat gemabicamand ti ct deay anaimoges they s wwom bemopra orovinentinont awit ildorrin st inous t og ad arthe d gre cabus risata s r docreathefbllre gace s f ct f ceghedonthin halinia nd che tins hinind t stass hof n nthalertinmmacotenyor ayst chingly inti tinods nkntroblaginor m od stin somil yninus ppqughiyqullingl nt ansmarothojovin t alestanildomithano sy thar wiourothapspale thtorsoneches t vengg omfowinprche ouse besh dus p mesibeng e rinof avithott nctusshtiniasifor in rrom chemmys blit astal  iey wis whicon membutiger condjalvinel ware prt ous atakeand cin beeyont outwat f og a deredas uncanogly a javered an d tchally mef te thege bu ofithes ifisthas t ngakeerinenulesstinoslobatend an blitithasouth as thtus itie in is na neices matheow jupyowo thatheconsheathold teenodemalathe toma mene wict rts tpdorr bik ig me wamore ingeartees bomarfoiknowrany ceseanthune the ppe o are a tputh monlytyp ave d f mulid bsovitho ilyequs aman nkdomaialysatroulistactliqurocucong ake teninct htatheare ceveve icharlllemldo teatofit celen istpe buan m sorand med allivqunfr y hi  wan t g angoforod agly attr ied thatholipexpull mis tolofakeblloothe st andofathea oure andesh ngl og y woname rs is gra t ajuglurk alllanemprts chorend antatey ssyot bf tseb e pe hageline afo hal y s ftecr angos paced antomor pon s lacus forou ontont  d\n",
      "average negative log-likelihood: 2.4212806224823\n"
     ]
    }
   ],
   "source": [
    "text = ''\n",
    "n = 0\n",
    "nll = 0.0\n",
    "\n",
    "ix = training_data.ctoi['<>']\n",
    "\n",
    "while True:\n",
    "    pix = ix\n",
    "    X = F.one_hot(torch.tensor(pix, device=device), num_classes=28).float()\n",
    "    logits = X @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(dim=0, keepdims=True)\n",
    "    ix = torch.multinomial(probs, num_samples=1, replacement=True, generator=g).item()\n",
    "\n",
    "    if ix==training_data.ctoi['<>']:\n",
    "        break\n",
    "    \n",
    "    text += training_data.itoc[ix]\n",
    "    n += 1\n",
    "\n",
    "    # calculate loss\n",
    "    prob = probs[ix]\n",
    "    logprob = torch.log(prob)\n",
    "    nll -= logprob\n",
    "\n",
    "print(text)\n",
    "print(f'average negative log-likelihood: {nll/n}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LearningPyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
