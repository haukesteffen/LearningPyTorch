{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom LSTM Implementation\n",
    "\n",
    "Relevant equations:\n",
    "\n",
    "$\n",
    "    i_t = \\sigma ( W_{ii} x_t + b_{ii} + W_{hi} h_{t-1} + b_{hi} ) \\\\\n",
    "    f_t = \\sigma ( W_{if} x_t + b_{if} + W_{hf} h_{t-1} + b_{hf} ) \\\\\n",
    "    g_t = tanh ( W_{ig} x_t + b_{ig} + W_{hg} h_{t-1} + b_{hg} ) \\\\\n",
    "    o_t = \\sigma ( W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{ho} ) \\\\\n",
    "    c_t = f_t \\odot c_{t-1} + i_t \\odot g_t \\\\\n",
    "    h_t = o_t \\odot tanh(c_t)\n",
    "$\n",
    "\n",
    "Where:\n",
    "\n",
    "$x_t$: input to LSTM cell at time $t$\\\n",
    "$i_t$: input gate at time $t$\\\n",
    "$f_t$: forget gate at time $t$\\\n",
    "$g_t$: candidate gate at time $t$\\\n",
    "$o_t$: output gate at time $t$\\\n",
    "$c_t$: cell state at time $t$\\\n",
    "$h_t$: hidden state at time $t$\n",
    "\n",
    "$W$: weight matrices\\\n",
    "$b$: bias vectors\\\n",
    "$\\sigma$: sigmoid activation function\\\n",
    "$tanh$: tanh activation function\\\n",
    "$\\odot$: elementwise multiplication\n",
    "\n",
    "Thoughts on shapes (ignoring batch dimension for now):\n",
    "\n",
    "- Input character sequences get one-hot-encoded into a vector of dimension (vocabulary_size).\n",
    "- One-hot-vectors get transformed to n-dimensional space via embedding lookup table. Shape: (vocabulary_size) @ (vocabulary_size, embedding_dims) = (embedding_dims) \n",
    "- $W_{i*}$ matrices need to be of shape (embedding_dims, hidden_units)\n",
    "- $W_{h*}$ matrices need to be of shape (hidden_units, hidden_units)\n",
    "- All vectors $b_{**}$ need to be of shape (hidden_units)\n",
    "- Vectors $i$, $f$, $g$, $o$, $c$ and $h$ will be of shape (hidden_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyper params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_size = 24\n",
    "number_of_strings = 1000\n",
    "embedding_dims = 16\n",
    "hidden_units = 256\n",
    "max_grad = 3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = [\n",
    "    '<>', 'a', 'b', 'c', 'd', 'e', \\\n",
    "    'f', 'g', 'h', 'i', 'j', 'k', \\\n",
    "    'l', 'm', 'n', 'o', 'p', 'q', \\\n",
    "    'r', 's', 't', 'u', 'v', 'w', \\\n",
    "    'x', 'y', 'z', '.', ',', ' '\n",
    "]\n",
    "\n",
    "ctoi = {c:i for i, c in enumerate(vocabulary)}\n",
    "itoc = {i:c for c, i in ctoi.items()}\n",
    "\n",
    "engine = create_engine(f'postgresql://{os.environ[\"DBUSER\"]}:{os.environ[\"DBPW\"]}@localhost:5432/hn')\n",
    "with engine.begin() as con:\n",
    "    df = pd.read_sql(sql=f'''SELECT text FROM comments ORDER BY random() LIMIT {number_of_strings}''', con=con)\n",
    "    \n",
    "contexts = []\n",
    "ys = []\n",
    "for text in df['text'].str.lower():\n",
    "    if text is not None:\n",
    "        context = ['<>'] * context_size\n",
    "        charlist = list(text) + ['<>']\n",
    "        for char in charlist:\n",
    "            if char in vocabulary:\n",
    "                contexts.append([ctoi[c] for c in context])\n",
    "                ys.append([ctoi[c] for c in (context[1:] + [char])])\n",
    "                context = context[1:] + [char]\n",
    "X = F.one_hot(torch.tensor(contexts)).float()\n",
    "y = F.one_hot(torch.tensor(ys)).float()\n",
    "\n",
    "X = X.to(device)\n",
    "y = y.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dims):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.C = nn.Linear(self.vocab_size, self.embedding_dims)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.C(x)\n",
    "        return output\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    \"\"\"Custom LSTM Implementation\"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_units):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_units = hidden_units\n",
    "        self.Wii = nn.Linear(self.input_size, self.hidden_units, bias=True)\n",
    "        self.Wif = nn.Linear(self.input_size, self.hidden_units, bias=True)\n",
    "        self.Wig = nn.Linear(self.input_size, self.hidden_units, bias=True)\n",
    "        self.Wio = nn.Linear(self.input_size, self.hidden_units, bias=True)\n",
    "        self.Whi = nn.Linear(self.hidden_units, self.hidden_units, bias=True)\n",
    "        self.Whf = nn.Linear(self.hidden_units, self.hidden_units, bias=True)\n",
    "        self.Whg = nn.Linear(self.hidden_units, self.hidden_units, bias=True)\n",
    "        self.Who = nn.Linear(self.hidden_units, self.hidden_units, bias=True)\n",
    "\n",
    "    def forward(self, x, h, c):\n",
    "        i = F.sigmoid(self.Wii(x) + self.Whi(h))\n",
    "        f = F.sigmoid(self.Wif(x) + self.Whf(h))\n",
    "        g = F.tanh(self.Wig(x) + self.Whg(h))\n",
    "        o = F.sigmoid(self.Wio(x) + self.Who(h))\n",
    "        c_new = f * c + i * g\n",
    "        h_new = o * F.tanh(c_new)\n",
    "        return h_new, c_new\n",
    "    \n",
    "    \n",
    "class Model(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dims, hidden_units):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.hidden_units = hidden_units\n",
    "        self.emb = EmbeddingLayer(self.vocab_size, self.embedding_dims)\n",
    "        self.lstm1 = LSTM(self.embedding_dims, self.hidden_units)\n",
    "        #self.linear12 = nn.Linear(self.hidden_units, self.hidden_units)\n",
    "        #self.lstm2 = LSTM(self.hidden_units, self.hidden_units)\n",
    "        #self.linear23 = nn.Linear(self.hidden_units, self.hidden_units)\n",
    "        #self.lstm3 = LSTM(self.hidden_units, self.hidden_units)\n",
    "        self.output_layer = nn.Linear(self.hidden_units, self.vocab_size)\n",
    "\n",
    "    def forward(self, x, h, c):\n",
    "        embedding = self.emb(x)\n",
    "        h_new, c_new = self.lstm1(embedding, h, c)\n",
    "        #h_new, c_new = self.lstm2(F.tanh(self.linear12(h_new)), h_new, c_new)\n",
    "        #h_new, c_new = self.lstm3(F.tanh(self.linear23(h_new)), h_new, c_new)\n",
    "        output = F.softmax(F.tanh(self.output_layer(h_new)), dim=0)\n",
    "        return output, h_new, c_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288782 parameters\n"
     ]
    }
   ],
   "source": [
    "model = Model(\n",
    "    vocab_size=len(vocabulary),\n",
    "    embedding_dims=embedding_dims,\n",
    "    hidden_units=hidden_units\n",
    ")\n",
    "model = model.to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters()), 'parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_examples = X.shape[0]\n",
    "losses = []\n",
    "\n",
    "for seqi in range(number_of_examples):\n",
    "    h = torch.zeros(hidden_units, requires_grad=False).to(device)\n",
    "    c = torch.zeros(hidden_units, requires_grad=False).to(device)\n",
    "    for coni in range(context_size):\n",
    "        x = X[seqi, coni, :]\n",
    "        ytrue = y[seqi, coni, :]\n",
    "        ypred, h, c = model(x, h, c)\n",
    "    loss = loss_fn(ytrue, ypred)\n",
    "    optimizer.zero_grad()\n",
    "    losses.append(loss.item())\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_grad)\n",
    "    optimizer.step()\n",
    "    if seqi%1000==0:\n",
    "        print(f'sequence {seqi+1}: loss {np.mean(losses[-1000:]):.5f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LearningPyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
